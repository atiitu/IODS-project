---
title: "Untitled"
author: "Anna Syreeni"
date: "7 joulukuuta 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Week6 LMM model Datacamp learning

Graphical displays of longitudinal data: The magical gather()

To be able to study the possible diffecences in the bprs value between the treatment groups and the possible change of the value in time, we don't want the weeks to be individual variables. In comes the gather() function which you can probably remember from previous chapters.

The gather() function takes multiple columns and collapses them into key-value pairs, so that we can have the weeks as values of a new variable week. You can find more information about gather in the package documentation with ?gather or in the dplyr cheatsheet.

Our weeks are in a bit inconvenient form as characters, so we somehow need to extract the week numbers from the character vector weeks.

With the substr() function we can extract a part of longer character object. We simply supply it with a character object or vector, start position, as in the position of the first letter to extract and stop position, as in the position of the last letter to extract. For example substr("Hello world!", 1, 5) would return "Hello".
Instructions
100 XP
Instructions
100 XP

    Factor variables treatment and subject
    Use gather() to convert BPRS to a long form
    Use mutate() and substr() to create column week by extracting the week number from column weeks
    Glimpse the data using glimpse()

Individuals on the plot

Graphical displays of data are almost always useful for exposing patterns in the data, particularly when these are unexpected; this might be of great help in suggesting which class of models might be most sensibly applied in the later more formal analysis.

To begin we shall plot the BPRS values for all 40 men, differentiating between the treatment groups into which the men have been randomized. This simple graph makes a number of features of the data readily apparent.

REMEMBER: In ggplot2 or dplyr syntax, you generally do not need to "quote" variable names!
Instructions
100 XP

    Draw the plot with week on the x-axis and bprs on the y-axis
    Inspect the plot. See how both the BPRS-score and the variability between individuals decrease over the eight weeks time

The Golden Standardise

An important effect we want to take notice is how the men who have higher BPRS values at the beginning tend to have higher values throughout the study. This phenomenon is generally referred to as tracking.

The tracking phenomenon can be seen more clearly in a plot of the standardized values of each observation, i.e., the values obtained by subtracting the relevant occasion mean from the original observation and then dividing by the corresponding visit standard deviation.

standardised(x)=x???mean(x)sd(x)

REMEMBER: In ggplot2 or dplyr syntax, you generally do not need to "quote" variable names!
Instructions
100 XP
Instructions
100 XP

    Assign week as the grouping variable
    Standardise the variable bprs
    Glimpse the data now with the standardised brps
    Plot the data now with the standardised brps




Good things come in Summary graphs

With large numbers of observations, graphical displays of individual response profiles are of little use and investigators then commonly produce graphs showing average (mean) profiles for each treatment group along with some indication of the variation of the observations at each time point, in this case the standard error of mean

se=sd(x)n?????????



    Create the summary data BPRSS with the mean and standard error of the variable bprs
    Glimpse the data
    Plot the mean profiles (with geom_errorbar() line commented out)
    Uncomment the geom_errorbar() line and plot the mean profiles again
    Note the considerable overlap in the mean profiles of the two treatment groups suggesting there might be little difference between the two groups in respect to the mean BPRS values

Find the outlaw... Outlier!

As an example of the summary measure approach we will look into the post treatment values of the BPRS. The mean of weeks 1 to 8 will be our summary measure. First calculate this measure and then look at boxplots of the measure for each treatment group. See how the mean summary measure is more variable in the second treatment group and its distribution in this group is somewhat skew. The boxplot of the second group also reveals an outlier, a subject whose mean BPRS score of the eight weeks is over 70. It might bias the conclusions from further comparisons of the groups, so we shall remove that subject from the data. Without the outlier, try to figure which treatment group might have the lower the eight-week mean. Think, considering the variation, how can we be sure?


T for test and A for Anova

Although the informal graphical material presented up to now has all indicated a lack of difference in the two treatment groups, most investigators would still require a formal test for a difference. Consequently we shall now apply a t-test to assess any difference between the treatment groups, and also calculate a confidence interval for this difference. We use the data without the outlier created in the previous exercise. The t-test confirms the lack of any evidence for a group difference. Also the 95% confidence interval is wide and includes the zero, allowing for similar conclusions to be made.

Baseline measurements of the outcome variable in a longitudinal study are often correlated with the chosen summary measure and using such measures in the analysis can often lead to substantial gains in precision when used appropriately as a covariate in an analysis of covariance. We can illustrate the analysis on the data using the BPRS value corresponding to time zero taken prior to the start of treatment as the baseline covariate. We see that the baseline BPRS is strongly related to the BPRS values taken after treatment has begun, but there is still no evidence of a treatment difference even after conditioning on the baseline value.


    Perform a two-sample t-test and observe the differences as seen in in the boxplots of the previous exercise
    Add the baseline from the original data as a new variable to the summary data
    Fit the linear model with mean as the target and baseline + treatment as the response from the BPRSL8S1 (Remember the lm() formula y ~ x1 + x2)
    Compute the analysis of variance table for the fitted model and pay close attention to the significance of baseline

Meet and Repeat: PART II

Welcome to the PART II of Analysis of longitudinal data chapter.

Longitudinal data, where a response variable is measured on each subject on several different occasions poses problems for their analysis because the repeated measurements on each subject are very likely to be correlated rather than independent. In PART II of this chapter methods for dealing with longitudinal data which aim to account for the correlated nature of the data and where the response is assumed to be normally distributed are discussed.

To investigate the use of linear mixed effects models in practice, we shall use data from a nutrition study conducted in three
groups of rats. The groups were put on different diets, and each animal's body weight (grams) was recorded repeatedly (approximately) weekly, except in week seven when two recordings were taken) over a 9-week period. The question of most interest is whether the growth profiles of the three
groups differ.



Plot first, ask questions later

To begin, we shall ignore the repeated-measures structure of the data and assume that all the observations are independent of one another. Now if we simply ignore that the sets of 11 weights come from the same rat, we have a data set consisting of 176 weights, times, and group memberships that we see can easily be analyzed using multiple linear regression. To begin, we will plot the data, identifying the observations in each group but ignoring the longitudinal nature of the data.

We'll start with a simple plot and continue by adding some styling elements. Feel free to experiment!


Holding on to independence: The Linear model

Continuing to ignore the repeated-measures structure of the data, we will fit a multiple linear regression model with weight as response and Time and Group as explanatory variables.

Recall again from Chapter 1: Multiple regression that this is done by defining explanatory variables with the formula argument of lm(), as below

y ~ x1 + x2 + ..

Here y is again the target variable and x1, x2, .. are the explanatory variables.

The Random Intercept Model

The previous model assumes independence of the repeated measures of weight, and this assumption is highly unlikely. So, now we will move on to consider both some more appropriate graphics and appropriate models.

To begin the more formal analysis of the rat growth data, we will first fit the random intercept model for the same two explanatory variables: Time and Group. Fitting a random intercept model allows the linear regression fit for each rat to differ in intercept from other rats.

We will use the lme4 package which offers efficient tools for fitting linear and generalized linear mixed-effects models. The first argument is the formula object describing both the fixed-effects and random effects part of the model, with the response on the left of a ~ operator and the terms, separated by + operators, on the right. Note the random-effects terms distinguished by vertical bars (|).

Slippery slopes: Random Intercept and Random Slope Model

Now we can move on to fit the random intercept and random slope model to the rat growth data. Fitting a random intercept and random slope model allows the linear regression fits for each individual to differ in intercept but also in slope. This way it is possible to account for the individual differences in the rats' growth profiles, but also the effect of time.
Slippery slopes: Random Intercept and Random Slope Model

Now we can move on to fit the random intercept and random slope model to the rat growth data. Fitting a random intercept and random slope model allows the linear regression fits for each individual to differ in intercept but also in slope. This way it is possible to account for the individual differences in the rats' growth profiles, but also the effect of time.

Time to interact: Random Intercept and Random Slope Model with interaction

Finally, we can fit a random intercept and slope model that allows for a group × time interaction.


    Write the same model as in the previous exercise but add Time * Group interaction.
    Print out the summary of the model
    Compute the analysis of variance tables of the models RATS_ref2 and RATS_ref1
    Again pay attention to the likelihood ratio test chi-squared value and the according p-value. The lower the value the better the fit against the comparison model.
    Draw the plot of observed values of RATSL (this is the same plot drawn earlier)
    Create a vector of the fitted values of the model using the function fitted()
    Use for example mutate() to add the vector Fitted as a new column to RATSL
    Draw the plot of fitted values of RATSL

