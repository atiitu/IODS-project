---
title: "Untitled"
author: "Anna Syreeni"
date: "21 marraskuuta 2018"
output: html_document
---

#Week4: Clustering and classification

##Exploring Boston-data##

For this week's excersize we'll use a Boston-dataset included in the MASS Package in R. Basically, the data is about per capita crime rate by town and some variables describing the towns. There are 14 variables on 506 towns included.

```{r, echo=FALSE, results=FALSE}
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
```

 
The variables in the Boston data are:<br/>
crim = per capita crime rate by town<br/>
zn = proportion of residential land zoned for lots over 25,000 sq.ft<br/> 
indus = proportion of non-retail business acres per town<br/> 
chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)<br/>
nox = nitrogen oxides concentration (parts per 10 million)<br/> 
rm  = average number of rooms per dwelling<br/>
age = proportion of owner-occupied units built prior to 1940<br/>
dis = weighted mean of distances to five Boston employment centres<br/>
rad = index of accessibility to radial highways<br/>
tax = full-value property-tax rate per \$10,000<br/>
ptratio = pupil-teacher ratio by town<br/>
black = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town<br/> 
lstat  = lower status of the population (percent)<br/>
medv = median value of owner-occupied homes in \$1000s<br/> 


The summaries of all variables are listed below. As seen from the summary, for example crime rate differs largely between towns: it ranges from 0.006 crimes from 89.0 crimes per capita.

```{r, echo=FALSE, results=TRUE}
summary(Boston)
```

```{r, echo=FALSE, results=FALSE, include=FALSE}
cor_matrix <- cor(Boston) 
round(cor_matrix, 2)
```

As seen from the correlation matrix, for example accessibility to radial highways (rad, calculated correlation r=0.63) and full-value propery-tax rate (tax, calculated r = 0.58) correlate positively with per capita crime rate. Living far Boston five Boston employment centres associates  with lower air nitric oxide concentration (correlation of dis and nos, r = -0.77)

```{r, echo=FALSE, results=FALSE, message=FALSE}
library(tidyverse)
library(corrplot)
corrplot(cor_matrix)
```

Next I standarsized all variables in the Boston data. Summaries of the variables are printed below. You can easily see that all standarized variables now have a mean of 0.

```{r, echo=FALSE, message=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)
```

For further analysis, a cathegorical variable from the scaled crime rate was created (quartiles). This variable now divides the crime rate to "low", "medium low", "medium high" and  "high" -cathegories.

```{r, echo=FALSE, message=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
#class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summary of the scaled crime rate
# summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
#bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, 
             label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
# table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Next the Boston data was divided into training and test sets. Randomly chosen 80% belongs to the training set, and the rest is in test-set.

```{r, echo=FALSE, message=FALSE, results=FALSE, cache=TRUE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
#n
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
#ind #random row numbers saved

# create train set
train <- boston_scaled[ind,]
#class(train) # now a smaller train data.frame was generated with random rows

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
library(dplyr)
test <- dplyr::select(test, -crime)
```

##Linear discrimant analysis##

Linear discrimant analysis of the Boston data shows that variable rad meaning accessibility to radial highways, is a very strong predictor for a town to belong to a group of high crime rate.


```{r, echo=FALSE, message=FALSE}
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
#lda.fit

# the function for lda biplot arrows
# arrow and text color set to black
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col="black", length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col="black", pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
# all was black before adding the col and pch
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

# lda.fit, correct_classes and test are available
#?predict.lda

# predict classes with test data
#lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
#table(correct = correct_classes, predicted = lda.pred$class)
```

The lda-model predicts very well the belonging to the high crime rate -cathegory.As seen from the propability table below, 100% of the observations in high- crime rate cathegory were correcly classified to the high crime rate group. 

For the other cathegories, LDA performs less well for other groups: For example, only 50% of towns in the true lowest crime-rate group were correctly classified. LDA had even more troubles in correcly classifing the medium high and medium low groups.


```{r, echo=FALSE, message=FALSE, results=TRUE, cache=TRUE}
# correct_classes already saved from the test-set and then removed
#?predict.lda

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
mytable <- table(correct = correct_classes, predicted = lda.pred$class)
mytable

prop.table(mytable, 1) %>% round(5)
```

##Distances in the Boston data and clustering##


```{r}
data("Boston")
#str(Boston)
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
#summary(boston_scaled)
```

Next, two distance measures were calculated for the Boston data:
Euclidean distance matrix and Manhattan distance matrix. The summaries of the distances can be found below.


```{r, include=FALSE}
# euclidean distance matrix, manhattan distance matrix
dist_eu <- dist(boston_scaled)
dist_man <- dist(boston_scaled, method = "manhattan")
```

**Euclidean distance matrix:**
```{r}
#summary of the distances
summary(dist_eu)
```

**Manhattan distance matrix:**
```{r}
#summary of the distances
summary(dist_man)
```

**K-means clustering**

For the k-means cluster, an optimal number of clusters can be decided from the figure below showing number of clusters (1-10) on the x-axis and within class sum of squares (**WCSS**) on the y-axis. As seen from the figure, 2 clusters seem to be the most optimal for clustring. 

```{r}
# k-means clustering
# MASS, ggplot2 and Boston dataset are available
# K-means might produce different results every time, because it randomly # assigns the initial cluster centers. The function set.seed() can be used # to deal with that.
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the WCSS results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```


Then k-means clustering was performed with 2 clusters (centers). From the figure we can see how well different variables are separated in different clusters (red or black). For example variables rm (average number of rooms per dwelling) and nox (nitric oxide concentration) form different groups for the two clusters. Especially variable crim (**crime rate in town**) totally separates the two groups. 


```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
dim(boston_scaled)
par(mfrow = c(2,1))
pairs(boston_scaled[,1:7], col = km$cluster)
pairs(boston_scaled[,8:14], col = km$cluster)
```
